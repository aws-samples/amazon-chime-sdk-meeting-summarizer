{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the Amazon Chime Meeting Summarizer documentation site.</p> <p>This docs provides an overview of the Chime Summarizer, a CDK application deploying infrastructure for a generative AI cross-platform call transcription and summarization bot. The bot is designed to be compatible with Amazon Chime, Cisco Webex, and Zoom. Leveraging Amazon Bedrock, Chime SDK, and Amazon Transcribe, it captures, transcribes, diarizes, and summarizes meeting audio to deliver comprehensive call summaries.</p> <p>Deploy Chime Summarizer</p> <pre><code>yarn launch\n</code></pre> <p></p> <p>The deployment will create in the account:</p> <ul> <li>the ReactJS frontend integrated with Amazon Cognito</li> <li>the Lambda functions to handle all the AWS Bedrock integrations</li> <li>the DynamoDB to store all the meetings metadata</li> <li>the API Gateway to handle all the frontend requests</li> </ul>"},{"location":"#benefits-of-the-chime-meeting-summarizer-solution","title":"Benefits of the <code>Chime Meeting Summarizer</code> solution","text":"<p>Enhance your meeting efficiency with the <code>Chime Meeting Summarizer</code>. This tool is designed to optimize your meeting management and information retrieval. Key features include:</p> <ul> <li> Speaker-Specific Transcription: Obtain detailed, speaker-identified transcripts of your meetings, ensuring clarity in who said what for better follow-up and accountability.</li> <li> Concise Meeting Summaries: Access quick, comprehensive summaries of each meeting, ideal for revisiting key points without needing to go through the entire transcript.</li> <li> Audio Playback Capability: Listen to the audio of your meetings at any time, useful for catching nuances that text transcriptions might miss.</li> <li> Interactive Chatbot for Quick Access: Use an AI-powered chatbot to efficiently search and retrieve specific information from your transcripts and summaries, aiding in quick responses to follow-up questions or clarifications post-meeting.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This getting started guide will walk you through setting up a new CDK project.</p>"},{"location":"getting-started/#pre-requisites","title":"Pre-requisites","text":"<p>Before diving into the project setup, make sure you have the following requirements in place:</p> <ul> <li>Yarn: Yarn must be installed on your machine. </li> <li>AWS Account: You'll need an active AWS account.</li> <li>Anthropic Models: These models should be enabled in your AWS Account. Learn more about enabling them here.</li> <li>Amazon Titan: Ensure that Amazon Titan is activated in your AWS Account. Learn more about enabling them here.</li> </ul>"},{"location":"getting-started/#setting-up-your-project","title":"Setting Up Your Project","text":"<p>To effectively utilize the <code>Amazon Chime SDK Meeting Summarizer</code>, your environment should have Node.js and Yarn pre-installed.</p> <p>Follow the below instructions to deploy the solution:</p>"},{"location":"getting-started/#deployment-process","title":"Deployment Process","text":"<p>Execute the following command in your terminal to deploy the project:</p> <pre><code>yarn launch\n</code></pre> <p>Example output:</p> <pre><code>yarn run v1.22.22\n$ npx projen launch\n\ud83d\udc7e launch | yarn &amp;&amp; yarn projen &amp;&amp; yarn build &amp;&amp; yarn cdk bootstrap &amp;&amp; yarn cdk deploy  --require-approval never &amp;&amp; yarn configLocal\n[1/4] \ud83d\udd0d  Resolving packages...\n[2/4] \ud83d\ude9a  Fetching packages...\n[3/4] \ud83d\udd17  Linking dependencies...\n[4/4] \ud83d\udd28  Building fresh packages...\nsuccess Saved lockfile.\n$ npx projen\n\ud83d\udc7e default | ts-node --project tsconfig.dev.json .projenrc.ts\n$ npx projen build\n\ud83d\udc7e build \u00bb default | ts-node --project tsconfig.dev.json .projenrc.ts\n\ud83d\udc7e build \u00bb post-compile \u00bb synth:silent | cdk synth -q\nBundling asset AmazonChimeSDKMeetingSummarizer/chimeSDKResources/smaHandlerLambda/Code/Stage...\nBundling asset AmazonChimeSDKMeetingSummarizer/Site/DeployBucket/Asset1/Stage...\n0.20.2\n[1/4] \ud83d\udd0d  Resolving packages...\nsuccess Already up-to-date.\n$ webpack --config webpack.config.js --mode production\nasset bundle.js 2.47 MiB [compared for emit] [minimized] [big] (name: main) 1 related asset\nasset index.html 648 bytes [compared for emit]\norphan modules 6.23 MiB [orphan] 2543 modules\nruntime modules 4.86 KiB 10 modules\nbuilt modules 4.92 MiB [built]\n  modules by path ./node_modules/ 3.81 MiB 565 modules\n  modules by path ./src/ 952 KiB\n    modules by path ./src/*.tsx 59.9 KiB 5 modules\n    modules by path ./src/styles/*.css 2.08 KiB 2 modules\n    modules by path ./src/pages/*.tsx 890 KiB 2 modules\n  asset modules 179 KiB\n    data:font/woff2;base64,d09GMgABAAAAAEE8.. 21.9 KiB [built] [code generated]\n    data:font/woff2;base64,d09GMgABAAAAAEV8.. 23.3 KiB [built] [code generated]\n    data:font/woff2;base64,d09GMgABAAAAAEE0.. 21.9 KiB [built] [code generated]\n    data:font/woff2;base64,d09GMgABAAAAAEVo.. 23.3 KiB [built] [code generated]\n    + 4 modules\nThis can impact web performance.\nAssets: \n  bundle.js (2.47 MiB)\nperformance.\nEntrypoints:\n  main (2.47 MiB)\n      bundle.js\nYou can limit the size of your bundles by using import() or require.ensure to lazy load some parts of your application.\nFor more info visit https://webpack.js.org/guides/code-splitting/\n\ud83d\udc7e build \u00bb test \u00bb eslint | eslint --ext .ts,.tsx --fix --no-error-on-unmatched-pattern  src test build-tools projenrc .projenrc.ts\n=============\nYou may find that it works just fine, or you may not.\n\nSUPPORTED TYPESCRIPT VERSIONS: &gt;=4.3.5 &lt;5.4.0\n\nYOUR TYPESCRIPT VERSION: 5.4.2\n\nPlease only submit bug reports when using the officially supported version.\n=============\n\u2728  Synthesis time: 27.68s\n\nAmazonChimeSDKMeetingSummarizer:  start: Building 840224d55457a9eb6799fac109422f47422243bf4badd7122f41eee2b28dbda2:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  success: Built 840224d55457a9eb6799fac109422f47422243bf4badd7122f41eee2b28dbda2:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  start: Publishing 840224d55457a9eb6799fac109422f47422243bf4badd7122f41eee2b28dbda2:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  start: Building e48d87a0b5f074aada5304444e83b794b0b114f2a4eef83797bd5c7e4d041a63:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  success: Built e48d87a0b5f074aada5304444e83b794b0b114f2a4eef83797bd5c7e4d041a63:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  start: Building 0c3aaf7163151bd99b727b2fb4bc0acc75a12e21e23d5122a0aeaef6a4e2d8dc:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  success: Built 0c3aaf7163151bd99b727b2fb4bc0acc75a12e21e23d5122a0aeaef6a4e2d8dc:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  start: Publishing e48d87a0b5f074aada5304444e83b794b0b114f2a4eef83797bd5c7e4d041a63:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  start: Publishing 0c3aaf7163151bd99b727b2fb4bc0acc75a12e21e23d5122a0aeaef6a4e2d8dc:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  success: Published e48d87a0b5f074aada5304444e83b794b0b114f2a4eef83797bd5c7e4d041a63:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  success: Published 840224d55457a9eb6799fac109422f47422243bf4badd7122f41eee2b28dbda2:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer:  success: Published 0c3aaf7163151bd99b727b2fb4bc0acc75a12e21e23d5122a0aeaef6a4e2d8dc:&lt;ACCOUNT-ID&gt;-us-east-1\nAmazonChimeSDKMeetingSummarizer: deploying... [1/1]\nAmazonChimeSDKMeetingSummarizer: creating CloudFormation changeset...\n\n \u2705  AmazonChimeSDKMeetingSummarizer\n\n\u2728  Deployment time: 161.49s\n\nOutputs:\nAmazonChimeSDKMeetingSummarizer.InfrastructuremeetingSummarizerAPIEndpoint42D7AD92 = https://&lt;API-GW-ID&gt;.execute-api.us-east-1.amazonaws.com/prod/\nAmazonChimeSDKMeetingSummarizer.collectionName = chatbot-knowledge-base-1cd6d7\nAmazonChimeSDKMeetingSummarizer.dataSourceId = DQLCRHFZAV\nAmazonChimeSDKMeetingSummarizer.knowledgeBaseId = KDVB0KKPIC\nAmazonChimeSDKMeetingSummarizer.knowledgeBaseRoleArn = arn:aws:iam::&lt;ACCOUNT-ID&gt;:role/AmazonChimeSDKMeetingSumm-BedrockKnowledgeBaseResou-sqMyOihXF5OH\nAmazonChimeSDKMeetingSummarizer.siteBucket = amazonchimesdkmeetingsumm-sitewebsitebucketbc20a56-&lt;BUCKET-ID&gt;\nAmazonChimeSDKMeetingSummarizer.summarizerSite = &lt;CLOUDFRONT-ID&gt;.cloudfront.net\nStack ARN:\narn:aws:cloudformation:us-east-1:&lt;ACCOUNT-ID&gt;:stack/AmazonChimeSDKMeetingSummarizer/47457510-ed8d-11ee-95b2-12152d099d29\n\n\u2728  Total time: 189.16s\n\n$ npx projen configLocal\n\ud83d\udc7e configLocal | aws s3 cp s3://$(yarn run --silent getBucket)/config.json site/public/\n\ud83d\udc7e getBucket | aws cloudformation describe-stacks --stack-name AmazonChimeSDKMeetingSummarizer --region us-east-1 --query 'Stacks[0].Outputs[?OutputKey==`siteBucket`].OutputValue' --output text\ndownload: s3://amazonchimesdkmeetingsumm-sitewebsitebucketbc20a56-&lt;BUCKET-ID&gt;/config.json to site/public/config.json\n</code></pre>"},{"location":"getting-started/#frontend-access","title":"Frontend Access","text":"<p>To begin scheduling your meetings through the frontend, simply utilize the AWS CloudFront URL provided upon the completion of the CDK deployment process. This URL serves as your access point to the user-friendly meeting scheduling interface.</p> <p></p>"},{"location":"getting-started/#clean-up-process","title":"Clean-up Process","text":"<p>To remove the deployed resources and clean up your environment, use:</p> <pre><code>yarn cdk destroy\n</code></pre> <p>This guide aims to provide a clear path for setting up your CDK project. Should you encounter any issues or have questions, feel free to open an issue here.</p>"},{"location":"usage/diarization/","title":"Speaker Diarization","text":""},{"location":"usage/diarization/#prompt-engineering-for-speaker-diarization","title":"Prompt Engineering for Speaker Diarization","text":"<p>Due to the limitation of speaker names in call audio collected via the dial-in feature, Amazon Bedrock infers speaker names based on conversation context. Prompt engineering helps identify speakers, returning a JSON object with speaker information.In the following code, we create the prompt which will be passed into the model. </p> <pre><code>const createPrompt = (transcript: string): string =&gt; {\n  const prompt = `Human: You are a meeting transcript names extractor. Go over the transcript and extract the names from it. Use the following instructions in the &lt;instructions&gt;&lt;/instructions&gt; xml tags\n  &lt;transcript&gt; ${transcript} &lt;/transcript&gt;\n  &lt;instructions&gt;\n  - Extract the names like this example - spk_0: \"name1\", spk_1: \"name2\".\n  - If no name is found for a speaker, use UNKNOWN_X where X is the speaker label number\n  - Only extract the names like the example above and do not add any other words to your response\n  - Your response should only have a list of \"speakers\" and their associated name separated by a \":\" surrounded by {}\n  - if there is only one speaker identified then surround your answer with {}\n  - the format should look like this {\"spk_0\" : \"Name\", \"spk_1: \"Name2\", etc.}, no unnecessary spacing should be added\n  &lt;/instructions&gt;\n\n  Assistant: Should I add anything else in my answer?\n\n  Human: Only return a JSON formatted response with the Name and the speaker label associated to it. Do not add any other words to your answer. Do NOT EVER add any introductory sentences in your answer. Only give the names of the speakers actively speaking in the meeting. Only give the names of the speakers actively speaking in the meeting in the format shown above.\n\nAssistant:`;\n  return JSON.stringify({\n    anthropic_version: 'bedrock-2023-05-31',\n    max_tokens: 10000,\n    messages: [\n        {\n        role: 'user',\n        content: [{ type: 'text', text: prompt }],\n        },\n    ],\n    temperature: 0.0,\n    });\n};\n</code></pre>"},{"location":"usage/limitations/","title":"Limitations","text":""},{"location":"usage/limitations/#limitations","title":"Limitations","text":"<ul> <li>For optimal results, speakers are encouraged to use microphones or headphone speakers.</li> <li>Amazon Transcribe supports speaker diarization for up to 10 speakers.</li> <li>If speaker names are not mentioned, Bedrock will return an \"Unknown\" tag</li> </ul>"},{"location":"usage/scheduling-meetings/","title":"Scheduling a Meeting with the SDK Summarizer Bot","text":"<p>Easily integrate the <code>Chime Summarizer Bot</code> into your meetings, whether they are about to start or scheduled for the future. The scheduler frontend efficiently manages both scenarios.</p>"},{"location":"usage/scheduling-meetings/#supported-meeting-platforms","title":"Supported Meeting Platforms","text":"<ul> <li> Chime: Fully compatible.</li> <li> Zoom: Fully compatible.</li> <li> Google Meet: Fully compatible.</li> <li> Microsoft Teams: Fully compatible.</li> <li> Webex: Fully compatible.</li> </ul>"},{"location":"usage/scheduling-meetings/#instant-scheduling","title":"Instant Scheduling","text":"<p>To have the bot join your meeting immediately:</p> <ol> <li>Paste the meeting invite into the text area.</li> <li>Click the 'Start Now' button.</li> <li>A green flashbar indicates successful scheduling.</li> </ol> <p></p> <p>Shortly afterward, the bot will request to join your meeting.</p> <p></p>"},{"location":"usage/scheduling-meetings/#future-scheduling","title":"Future Scheduling","text":"<p>To schedule the bot for a future meeting:</p> <ol> <li>Paste the meeting invite into the text area.</li> <li>Check the 'Schedule for Later' box.</li> <li>Select the date on the calendar.</li> <li>Enter the time in the time box.</li> <li>Click the 'Schedule Future Meeting' button.</li> </ol> <p>A green flashbar confirms successful scheduling. The bot will join your meeting at the specified date and time, based on your browser's timezone.</p> <p></p> <p>View your upcoming meetings in the 'Your Future Meetings' section.</p> <p></p>"},{"location":"usage/scheduling/","title":"Call Scheduling","text":""},{"location":"usage/scheduling/#bot-scheduling-lambda","title":"Bot Scheduling Lambda","text":"<p>To enhance user experience, users can easily schedule the bot by copying and pasting a meeting invite into a form. Using prompt engineering in Bedrock, meeting details such as meetingID and meeting type are extracted and passed to the SIP media application Lambda. In the following code, we create the prompt which will be passed into the model. </p> <pre><code>//See full code in src/resources/requestprocessor/utils.ts\nconst createPrompt = (meetingInvitation: string): string =&gt; {\n  return JSON.stringify({\n    prompt: `Human: You are a an information extracting bot. Go over the meeting invitation below and determine what the meeting id and meeting type are &lt;instructions&gt;&lt;/instructions&gt; xml tags\n\n            &lt;meeting_invitation&gt;  \n                    ${meetingInvitation}\n            &lt;/meeting_invitation&gt; \n\n            &lt;instructions&gt;  \n\n          1. Identify Meeting Type:\n              Determine if the meeting invitation is for Chime, Zoom, Google, Microsoft Teams, or Webex meetings.\n\n          2. Chime, Zoom, and Webex\n              - Find the meetingID\n              - Remove all spaces from the meeting ID (e.g., #### ## #### -&gt; ##########). \n\n          2. If Google -  Instructions Extract Meeting ID and Dial in \n            - For Google only, the meeting invitation will call a meetingID a 'PIN', so treat it as a meetingID\n            - Remove all spaces from the PIN (e.g., #### ## #### -&gt; ##########). \n            - Extract Google the dialIn number\n            - Locate the dial-in number following the text \"otherwise, to join by phone\"\n            - Format the extracted Google dial-in number as (+1 ###-###-####), removing dashes and spaces. For example +1 111-111-1111 would become +11111111111)\n\n          3. If Microsoft Teams - Instructions if meeting type is is Microsoft Teams. \n            - Pay attention to these instructions carefully            \n            - The meetingId we want to store in the generated response is the 'Phone Conference ID' : ### ### ###\n            - in the meeting invitation, there are two IDs a 'Meeting ID' (### ### ### ##) and a 'Phone Conference ID' (### ### ###), ignore the 'Meeting ID' use the 'Phone Conference ID'\n            - The meetingId we want to store in the generated response is the 'Phone Conference ID' : ### ### ###\n            - Find the phone number, extract it and store it as the dialIn number (format (+1 ###-###-####), removing dashes and spaces. For example +1 111-111-1111 would become +11111111111)\n\n          4. meetingType rules\n          - The only valid responses for meetingType are 'Chime', 'Webex', 'Zoom', 'Google', 'Teams'\n\n          5. meetingId Format Rules \n\n          Zoom: ### #### ####\n          Webex: #### ### ####\n          Chime: #### ## ####\n          Google: ### ### #### (last character is always '#')\n          Teams: ### ### ###\n\n          6. Other notes\n          - Ensure that the program does not create fake phone numbers and only includes the Microsoft or Google dial-in number if the meeting type is Google or Teams.\n          - Ensure that the meetingId matches perfectly.\n          - If present extract a \"meeting title\" and store it in the FINAL JSON Response as \"meetingTitle\"\n          - If no title is detected then store the value of \"No Title Detected In Invite\"\n\n\n          7.    Generate FINAL JSON Response:\n\n              - Create a response object with the following format:\n              { \n                meetingId: \"meeting id goes here with spaces removed\",\n                meetingType: \"meeting type goes here (options: 'Chime', 'Webex', 'Zoom', 'Google', 'Teams')\",\n                dialIn: \"Insert Microsoft or Google Dial-In number with no dashes or spaces, or N/A if not a Google Meeting or Teams Meeting\"\n                meetingTitle: \"Insert Extracted Meeting Title or return 'No Title Detected in Invite\",\n              }\n\n              Meeting ID Formats:\n\n\n          &lt;/instructions&gt;\n\n          Assistant: Should I add anything else in my answer?\n\n          Human: Only return a JSON formatted response with the meetingid and meetingtype associated to it. Do not add any other words to your answer. Do not add any introductory sentences in your answer.    \\nAssistant:`,\n    max_tokens_to_sample: 100,\n    temperature: 0,\n  });\n};\n</code></pre>"},{"location":"usage/summarization/","title":"Summarization","text":""},{"location":"usage/summarization/#prompt-engineering-for-speaker-summarization","title":"Prompt Engineering for Speaker Summarization","text":"<p>Transcripts are converted into digestible meeting summaries using prompt engineering in Amazon Bedrock. This process generates concise summaries, including follow-up tasks discussed during the call, with the flexibility to customize focus areas. In the following code, we create the prompt which will be passed into the model. In the following code, we create the prompt which will be passed into the model. </p> <pre><code>const createPrompt = (transcript: string): string =&gt; {\n\n  const prompt = `Human: You are a transcript summarizing bot. You will go over the transcript below and provide a summary of the content within the &lt;instructions&gt; tags.\n\n  &lt;transcript&gt; ${transcript} &lt;/transcript&gt;\n\n    &lt;instructions&gt; \n    - Generate the summary in the language that the transcript is in. \n    - Go over the conversation that was had in the transcript. \n    - Create a summary based on what occurred in the meeting. \n    - Highlight specific action items that came up in the meeting, including follow-up tasks for each person. \n    - If relevant, focus on what specific AWS services were mentioned during the conversation. \n    - If there's sufficient context, infer the speaker's role and mention it in the summary. For instance, \"Bob, the customer/designer/sales rep/...\" \n    &lt;/instructions&gt;\n\n  Assistant:\n\n    Assistant: Should I add anything else in my answer?\n\n    Human: No matter the length of the transcript, summarize what happened. Do not include any xml tags &lt;&gt;   \\nAssistant:`;\n  return JSON.stringify({\n    anthropic_version: 'bedrock-2023-05-31',\n    max_tokens: 10000,\n    messages: [\n        {\n        role: 'user',\n        content: [{ type: 'text', text: prompt }],\n        },\n    ],\n    });\n};\n</code></pre>"},{"location":"usage/transcription/","title":"Transcription","text":""},{"location":"usage/transcription/#prompt-engineering-for-transcript-cleaning","title":"Prompt Engineering for Transcript Cleaning","text":"<p>Using prompt engineering, we can improve the quality of the diarization and also perform tasks such as the removal of filler words. Refer to the prompt below to see how we improved the quality of the transcript. </p> <p><code>``typescript //See full code in src/resources/cleanTranscript const createPayload = (transcript: string): string =&gt; {     const prompt =</code>Human: You are a transcript editor, please follow the  tags. <pre><code>&lt;transcript&gt; ${transcript} &lt;/transcript&gt;\n\n    &lt;instructions&gt; \n    - The &lt;transript&gt; contains a speaker diarized transcript \n    - Go over the transcript and remove all filler words. For example  \"um, uh, er, well, like, you know, okay, so, actually, basically, honestly, anyway, literally, right, I mean.\"\n    - Fix any errors in transcription that may be caused by homophones based on the context of the sentence.  For example, \"one instead of won\" or \"high instead of hi\"\n    - In addition, please fix the transcript in cases where diarization is improperly performed. For example, in some cases you will see\n    that sentences are split between two speakers. In this case infer who the actual speaker is and attribute it to them. \n    - Please review the following example of this,\n\n    Input Example\n    Chris: Adam you are saying the wrong thing. What \n    Adam: um do you mean, Chris?\n\n    Output: \n    Chris: Adam you are saying the wrong thing.\n    Adam: What do you mean, Chris?\n\n    - In your response, return the entire cleaned transcript, including all of the filler word removal and the improved diarization. Only return the transcript, do not include any leading or trailing sentences. You are not summarizing. You are cleaning the transcript. Do not include any xml tags &lt;&gt;\n    &lt;/instructions&gt;\n\nAssistant:`;\nreturn JSON.stringify({\n    anthropic_version: 'bedrock-2023-05-31',\n    max_tokens: 10000,\n    messages: [\n        {\n        role: 'user',\n        content: [{ type: 'text', text: prompt }],\n        },\n    ],\n    });\n};\n</code></pre> <p>```</p>"}]}